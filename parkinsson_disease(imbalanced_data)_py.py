# -*- coding: utf-8 -*-
"""parkinsson_disease(imbalanced_data).py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10TC9pgfnLs0XlGdiDOz4Pazjfy0UtMrg
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv(r'/content/Parkinsson disease.csv')
df

df.head()

df.tail()

df['status'].value_counts()

df.shape

df.info()

df.nunique()

df.isnull().sum()

df.describe()

x=df.drop('status',axis=1)
x.head()

y=df['status']
y.head()

df.drop('name',axis=1,inplace=True)

df.groupby('status').mean()

plt.figure(figsize=(2,5))
sns.countplot(x='status',data=df)
plt.title('Status')
plt.show()

plt.figure(figsize=(10,8))
plt.subplot(1,2,1)
sns.barplot(x='status',y='NHR',data=df)
plt.title('NHR')
plt.subplot(1,2,2)
sns.barplot(x='status',y='HNR',data=df)
plt.title('HNR')
plt.show()

plt.figure(figsize=(10,8))
df.boxplot()
plt.title('Boxplot')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12,15))
for i, col in enumerate(df):
    plt.subplot(5,5,i+1)
    # Use the column name directly for the 'x' argument
    sns.boxplot(x=col, data=df)
    #plt.title(f' {col}')
plt.tight_layout()
plt.show()

print("\nFeature Analysis:")
df.hist(bins=15, figsize=(20, 15))
plt.show()

corr_matrix = df.corr()
plt.figure(figsize=(15, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

#sns.pairplot(df)
#plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

import warnings
warnings.filter=warnings.filterwarnings('ignore')

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

models=[[LogisticRegression(),RandomForestClassifier(),SVC(),KNeighborsClassifier(),GaussianNB(),DecisionTreeClassifier()]]

x_train.drop('name',axis=1,inplace=True)
x_test.drop('name',axis=1,inplace=True)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.transform(x_test)

for model_list in models:  # Iterate over the outer list
    for model in model_list:  # Iterate over the inner list containing models
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)  # Use scaled test data
        print(f'{model} \n"accuracy score": {accuracy_score(y_test, y_pred)}')
        print("confusion matrics\n", confusion_matrix(y_test, y_pred))
        print(classification_report(y_test, y_pred))
        print('\n')
        print('*' * 15)

from sklearn.model_selection import KFold, cross_val_score,ShuffleSplit,GridSearchCV,RandomizedSearchCV,StratifiedKFold,LeaveOneOut

model_1=RandomForestClassifier()
grid={
    'criterion':['gini','entropy'],
    'max_depth':[2,4,6,8,10],
    'min_samples_split':[1,5,10],
    'max_features':['sqrt','log2'],
    'random_state':[5,10,15,20]
}
gscv=GridSearchCV(estimator=model_1,param_grid=grid,cv=5)
gscv.fit(x_train,y_train)

print("RandomForestclassifier best score :-",gscv.best_score_)
gscv.best_params_

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE,ADASYN,RandomOverSampler

sampler=[('smote',SMOTE()), ('adasyn',ADASYN()),('randomoversampler',RandomOverSampler())]

model=[('logistic regression',LogisticRegression()),('decisiontreeclassifier',DecisionTreeClassifier(max_depth=8,min_samples_split=2,criterion="entropy",random_state=0)),("randomforestclassifier",RandomForestClassifier(n_estimators=100,criterion="gini",min_samples_split=3,max_depth=12,random_state=0)),("bagging classifier",BaggingClassifier(n_estimators=50,random_state=20)),("support vector classifier",SVC(kernel="linear"))]

for rs_name,resampler in sampler:
    for model_name,modeler in model:
        pipe=Pipeline([('scaling',StandardScaler()),('resampler',resampler),('modeler',modeler)])
        pipe.fit(x_train,y_train)
        pred=pipe.predict(x_test)
        print("Sampler:",rs_name)
        print("Model name:",model_name)
        print("Accuracy score:",accuracy_score(y_test,pred))
        print("Classification report:",classification_report(y_test,pred))
        print("----------"*15)

sm=SMOTE()
x_rs,y_rs=sm.fit_resample(x_train,y_train)
rfc=RandomForestClassifier(max_depth=8,min_samples_split=5,criterion="entropy",random_state=5)
rfc.fit(x_rs,y_rs)
pred=rfc.predict(x_test)
print(classification_report(y_test,pred))

c_m=confusion_matrix(y_test,pred)
sns.heatmap(c_m,annot=True)
plt.ylabel("Actual values")
plt.xlabel("Predicted Values")
plt.show()

